{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c9e54ef-afa1-46d6-aace-b0fd3499f86e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.6977 - loss: 0.6038 - val_accuracy: 0.8618 - val_loss: 0.3896\n",
      "Epoch 2/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8925 - loss: 0.3346 - val_accuracy: 0.8834 - val_loss: 0.3029\n",
      "Epoch 3/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9178 - loss: 0.2395 - val_accuracy: 0.8864 - val_loss: 0.2874\n",
      "Epoch 4/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9333 - loss: 0.1936 - val_accuracy: 0.8940 - val_loss: 0.2700\n",
      "Epoch 5/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9462 - loss: 0.1648 - val_accuracy: 0.8716 - val_loss: 0.3289\n",
      "Epoch 6/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9495 - loss: 0.1486 - val_accuracy: 0.8844 - val_loss: 0.2984\n",
      "Epoch 7/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9609 - loss: 0.1224 - val_accuracy: 0.8568 - val_loss: 0.3991\n",
      "Epoch 8/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9588 - loss: 0.1183 - val_accuracy: 0.8850 - val_loss: 0.3235\n",
      "Epoch 9/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0954 - val_accuracy: 0.8832 - val_loss: 0.3318\n",
      "Epoch 10/10\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0820 - val_accuracy: 0.8602 - val_loss: 0.4507\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 782us/step - accuracy: 0.8470 - loss: 0.4722\n",
      "Precisión en conjunto de prueba: 0.8500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 609us/step\n",
      "\n",
      "--- Crítica #1 (Palabras clave resaltadas) ---\n",
      "<START> please give this one a miss br br <UNK> <UNK> and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite <UNK> so all you madison fans give this a miss\n",
      "\n",
      "Etiqueta real: Negativa\n",
      "Predicción del modelo: Negativa\n",
      "\n",
      "--- Crítica #2 (Palabras clave resaltadas) ---\n",
      "<START> this film requires a lot of patience because it focuses on mood and character development the plot is very simple and many of the scenes take place on the same set in frances <UNK> the sandy dennis character apartment but the film builds to a disturbing climax br br the characters create an atmosphere <UNK> with sexual tension and psychological <UNK> it's very interesting that robert altman directed this considering the style and structure of his other films still the trademark altman audio style is evident here and there i think what really makes this film work is the brilliant performance by sandy dennis it's definitely one of her darker characters but she plays it so perfectly and convincingly that it's scary michael burns does a good job as the mute young man regular altman player michael murphy has a small part the <UNK> moody set fits the content of the story very well in short this movie is a powerful study of loneliness sexual <UNK> and desperation be patient <UNK> up the atmosphere and pay attention to the wonderfully written script br br i praise robert altman this is one of his many films that deals with unconventional fascinating subject matter this film is disturbing but it's sincere and it's sure to <UNK> a strong emotional response from the viewer if you want to see an unusual film some might even say bizarre this is worth the time br br unfortunately it's very difficult to find in video stores you may have to buy it off the internet\n",
      "\n",
      "Etiqueta real: Positiva\n",
      "Predicción del modelo: Positiva\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Definir la cantidad de palabras a considerar en el dataset\n",
    "num_words = 10000  # Puedes cambiar este valor fácilmente\n",
    "\n",
    "# 1. Cargar el dataset IMDB con la cantidad de palabras definida\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "# Obtener el diccionario de palabras\n",
    "word_index = imdb.get_word_index()\n",
    "word_index = {k: (v + 3) for k, v in word_index.items()}  # Ajuste de índices\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # Desconocidas\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "reverse_word_index = {v: k for k, v in word_index.items()}  # Invertir diccionario\n",
    "\n",
    "# 2. Preprocesar los datos: convertir listas de enteros en tensores\n",
    "def vectorize_sequences(sequences, dimension=num_words):\n",
    "    results = np.zeros((len(sequences), dimension))  # Matriz de ceros\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.0  # Poner 1 en los índices correspondientes a las palabras\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data, dimension=num_words)\n",
    "x_test = vectorize_sequences(test_data, dimension=num_words)\n",
    "\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# 3. Construir la red neuronal\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation=\"relu\", input_shape=(num_words,)),  # Capa oculta 1\n",
    "    layers.Dense(16, activation=\"relu\"),  # Capa oculta 2\n",
    "    layers.Dense(1, activation=\"sigmoid\")  # Capa de salida (clasificación binaria)\n",
    "])\n",
    "\n",
    "# 4. Compilar el modelo\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# 5. Entrenar el modelo (validando con 20% de los datos de entrenamiento)\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_split=0.2)\n",
    "\n",
    "# 6. Evaluar el modelo en el conjunto de prueba\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(f\"Precisión en conjunto de prueba: {results[1]:.4f}\")\n",
    "\n",
    "# 7. Obtener predicciones y construir la matriz de confusión\n",
    "y_pred_prob = model.predict(x_test)\n",
    "y_pred = (y_pred_prob > 0.5).astype(\"int32\")  # Convertir probabilidades en 0 o 1\n",
    "\n",
    "# Calcular la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Graficar la matriz de confusión\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Negativa\", \"Positiva\"], yticklabels=[\"Negativa\", \"Positiva\"])\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Etiqueta real\")\n",
    "plt.title(\"Matriz de Confusión - Clasificación IMDB\")\n",
    "plt.show()\n",
    "\n",
    "# 8. Mostrar reporte de clasificación\n",
    "print(\"\\nReporte de clasificación:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 9. Mostrar las dos primeras críticas del conjunto de prueba con más palabras reconocidas\n",
    "def decode_review(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"<UNK>\") for i in text])\n",
    "\n",
    "# Mostrar la primera y segunda crítica\n",
    "for i in range(2):\n",
    "    sample_review = decode_review(test_data[i])  # Convertir de enteros a palabras\n",
    "    real_label = \"Positiva\" if test_labels[i] == 1 else \"Negativa\"\n",
    "    predicted_label = \"Positiva\" if y_pred[i] == 1 else \"Negativa\"\n",
    "\n",
    "    print(f\"\\n--- Crítica #{i+1} ---\")\n",
    "    print(sample_review)\n",
    "    print(f\"\\nEtiqueta real: {real_label}\")\n",
    "    print(f\"Predicción del modelo: {predicted_label}\")\n",
    "\n",
    "# 10. Graficar la pérdida y la precisión durante el entrenamiento\n",
    "history_dict = history.history\n",
    "\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "acc_values = history_dict[\"accuracy\"]\n",
    "val_acc_values = history_dict[\"val_accuracy\"]\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "# Pérdida\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Precisión\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, acc_values, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc_values, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "def highlight_important_words(model, review_vector, review_text):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(review_vector)\n",
    "        prediction = model(review_vector)\n",
    "    \n",
    "    gradients = tape.gradient(prediction, review_vector).numpy().flatten()\n",
    "    \n",
    "    # Obtener las palabras con mayor importancia según los gradientes\n",
    "    importance = np.abs(gradients)\n",
    "    word_importance = {reverse_word_index.get(i, \"<UNK>\"): importance[i] for i in range(len(importance)) if i in reverse_word_index}\n",
    "    \n",
    "    # Ordenar palabras por importancia\n",
    "    sorted_words = sorted(word_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Seleccionar las palabras más importantes\n",
    "    top_words = {word for word, _ in sorted_words[:10]}  # Top 10 palabras\n",
    "    \n",
    "    # Resaltar palabras clave en la crítica\n",
    "    highlighted_review = \" \".join([f\"**{word}**\" if word in top_words else word for word in review_text.split()])\n",
    "    \n",
    "    return highlighted_review"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
